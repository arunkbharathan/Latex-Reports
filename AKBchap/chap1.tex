%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Introduction}

\section{Background}
A brain-computer interface (BCI) is a device that can read brain signals and convert them into control and communication signals. BCIs are often directed at assisting, augmenting, or repairing human cognitive or sensory-motor functions. A standard BCI system consists of a signal acquisition, signal enhancement, feature extraction, classification and the control interface stages as shown in Fig.~\ref{int}. This chapter gives a basic introduction to BCI, types of neuroimaging modalities used in the signal acquisition step and types of electro-physiological control signals that determine the user intentions.

BCI design represents a new frontier in science and technology that requires multidisciplinary skills from fields such as neuroscience, engineering, computer science, psychology and clinical rehabilitation to achieve the goal of developing an alternative communication medium. Despite the technological developments, there remain numerous obstacles to build an efficient BCI. The biggest challenges are related to accuracy, speed and usability. If a disabled person can move his/her eyes or even a single muscle in a controlled way, the interfaces based on eye-gaze or EMG switch technology are more efficient than any of the BCIs that exist today. The maximum information transfer rate of current BCI systems is typically 25 bits per minute.

\begin{figure}
\centering
\includegraphics[scale=0.8]{Figure1/bci1.png}
\caption{Brain Computer Interface}
\label{int}
\end{figure}
    
The past two decades have seen an explosion of scientific interest in a completely different and novel approach of interacting with a computer. Inspired by the social recognition of people who suffer from severe neuromuscular disabilities, an interdisciplinary field of research has been created to offer direct human computer interaction via signals generated by the brain itself. BCI technology, as it is known, is a revolutionary communication channel that enables users to control computer applications through thoughts alone. The development of the cognitive neuroscience field has been instigated by recent advances in brain imaging technologies such as Electroencephalography (EEG), Magnetoencephalography (MEG) and functional magnetic resonance imaging (fMRI). 

EEG is an imperfect and distorted indicator of brain activity, yet the fact that it can be acquired cheaply, is non-invasive and demonstrates direct functional correlations with high temporal resolution makes it the only practical direct brain computer communication channel. It is a new and challenging medium for us to exploit in a similar manner to the other communication modalities such as voice or vision. The endless potential of tapping into human brain signals may see the fantasies of science fiction writers becoming reality in the future.

Many complex processes and systems would operate on the basis of thought in the future. Currently, the field of BCI is in infancy stage and would require deeper insights on how to capture the right signals and then process them suitably. The advancements are limited to recognition of certain words, expressions, moods etc. Efforts are being made to recognize the objects as they are seen by the brain. These efforts will bring in newer dimensions in the understanding of brain functioning, damage and repair. It is possible to recognize the thoughts of the human brain by capturing the right signals from the brain in future. But present operating systems and interfaces are not suitable for working with thought based system. In future new OS may be developed for the BCI applications.

The focus of this thesis is on the signal enhancement or pre-processing, feature extraction and feature classification stage. The BCI competition data sets were used for the analysis of algorithm. A combined feature extraction and feature classification method is employed in our work. 



\section{Motivation and Problem Statement}

Focusing on the EEG as the BCI input modality, the goal was to develop a deep understanding of the neurophysiological processes that could be exploited to implement a BCI system. After performing a state-of-the-art review of BCI systems, it was envisaged to design and implement a system. A basic knowledge of EEG waveform characteristics, signal processing methodologies for feature extraction and classification is a prerequisite before attempting to design and implement a BCI system. 

The common spatial pattern (CSP) method is used for the extraction of motor imagery tasks in BCI. It is followed by a classification stage using linear discriminant analysis stage. In CSP, the usual notion is to take a pair of eigen vectors from both the sides of coefficient matrix. This is not an optimal method for noise contaminated signals. An alternate method to overcome this problem is to use a discriminative model which captures a low rank structure controlled by regularization and thereby achieving a simple classification. This regularizer reduces singular values of the coefficient matrix.

\section{Aim and Objectives}

Based on a short segment of EEG called a trial, the signal analysis in BCI aims to predict the brain state of a user out of prescribed options (e.g. foot Vs. left-hand motor imagery Vs. rest). In machine learning terms, this is a multi-class classification problem. The challenge in EEG-based BCI is the low spatial resolution caused by volume conduction, the high artefact and the outlier content of the signal and the mass of data that makes the application of conventional statistical analysis difficult. Therefore many studies have focused on how to extract a small number of task informative features from the data that can be fed into some relatively simple classifiers; commonly used are linear spatial filtering methods or independent component analysis coupled with heuristic frequency band selection or band weighting. 

One of the shortcomings of the feature extraction approaches is the strong and hard-to-control inductive bias. It limits their application to rather specific experimental paradigms for which they are developed. Another approach is the discriminative approach that tries to optimize the classifier coefficients from the training data under a unified criterion. The theoretical advantage of the discriminative approach is that the coefficients (e.g., spatial filter and temporal filter) are jointly optimized under a single criterion. Moreover, inductive bias can be controlled in a principled manner through regularization. However many previous studies had to solve non-convex optimization problems, which can be challenging because of multiple local minima and difficulty in terminating the learning algorithms.

Here, we combine the probabilistic data-fit criteria with sparse regularizers. The proposed regularizers naturally induce sparse or factorized models through a convex optimization problem; moreover the number of components is automatically determined; in addition it is shown that the decoding model can be instantly converted into a loss function that is used for the training of the classifier; thus no intermediate goal such as binary classification needs to be imposed. Finally, it has been shown that the different second order informations in the signal can be combined and selected in a systematic manner through the dual spectral (DS) regularization. The issue of complexity control, feature extraction, and the interpretability of the resulting model is thus tackled in a unified and systematic manner under the roof of a convex regularized empirical risk minimization problem.


\section{Thesis Outline}

This thesis presents the fundamental knowledge behind developing an Electroencephalogram based BCI as well as a state-of-the-art review of BCI research. The thesis concludes by looking in to the future of BCI technology.  

The \textbf{Chapter two} aims at reviewing the main BCI designs and their applications. This chapter starts by giving some definitions related to BCI. Then it reviews the methods and techniques used to design a BCI. As such, it details with the different processing steps composing a BCI, i.e., measurements of brain activity, preprocessing, feature extraction and classification. Finally, some BCI applications and the already developed prototypes  are discussed, by emphasising virtual reality applications.

The \textbf{Chapter three} addresses the problem of feature extraction and classification. CSP method for feature extraction and LDA method for classification.

\textbf{Chapter four} presents the technique of one step feature extraction and classification, its algorithm and implementation. 

\textbf{Chapter five} deals with results, discussion and modification of the technique. 

The conclusion in \textbf{Chapter six} briefly summarizes the outcome of the thesis and provides suggestions for future works. 




%\section{Motivations for micro-optimization}

%The idea of micro-optimization is motivated by the recent trends in computer
%architecture towards low-level parallelism and small, pipelineable
%instruction sets \cite{patterson:risc,rad83}.  By getting rid of more
%complex instructions and concentrating on optimizing frequently used
%instructions, substantial increases in performance were realized.


%\cite{ellis:bulldog,pet87,coutant:precision-compilers}.  In these cases, the
%compiler not only is responsible for faithfully generating native code to
%match the source language, but also must be aware of instruction latencies,
%delayed branches, pipeline stages, and a multitude of other factors in order
%to generate fast code \cite{gib86}.

%These optimizations are described in detail in section~\ref{ch1:opts}.

%\section{Description of micro-optimization}\label{ch1:opts}

%A discussion of the mathematics behind unnormalized arithmetic is in appendix~\ref{unnorm-math}.

% The CDC 6600 had all of its instructions performing unnormalized arithmetic, with a separate {\tt NORMALIZE} instruction.}.

% This is an example of how you would use tgrind to include an example
% of source code; it is commented out in this template since the code
% example file does not exist.  To use it, you need to remove the '%' on the
% beginning of the line, and insert your own information in the call.
%
%\tagrind[htbp]{code/pmn.s.tex}{Post Multiply Normalization}{opt:pmn}

%As you can see, the intermediate results can be multiplied together, with noneed for intermediate normalizations due to the guard bit.  It is only atthe end of the operation that the normalization must be performed, in orderto get it into a format suitable for storing in memory\footnote{Note thatfor purposed of clarity, the pipeline delays were considered to be 0, andthe branches were not delayed.}.

%\subsection{Block Exponent}

%In a unoptimized sequence of additions, the sequence of operations is asfollows for each pair of numbers ($m_1$,$e_1$) and ($m_2$,$e_2$).
%\begin{enumerate}
 % \item Compare $e_1$ and $e_2$.
  %\item Shift the mantissa associated with the smaller exponent $|e_1-e_2|$
   %     places to the right.
  %\item Add $m_1$ and $m_2$.
  %\item Find the first one in the resulting mantissa.
  %\item Shift the resulting mantissa so that normalized
  %\item Adjust the exponent accordingly.
%\end{enumerate}

%Out of 6 steps, only one is the actual addition, and the rest are involved in aligning the mantissas prior to the add, and then normalizing the result afterward.  In the block exponent optimization, the largest mantissa is found to start with, and all the mantissa's shifted before any additions take place.  Once the mantissas have been shifted, the additions can take place one after another\footnote{This requires that for n consecutive additions, there are $\log_{2}n$ high guard bits to prevent overflow.  In the $\mu$FPU, there are 3 guard bits, making up to 8 consecutive additions possible.}.  An example of the Block Exponent optimization on the expression X = A + B + C is given in figure~\ref{opt:be}.

% This is an example of how you would use tgrind to include an example
% of source code; it is commented out in this template since the code
% example file does not exist.  To use it, you need to remove the '%' on the
% beginning of the line, and insert your own information in the call.
%
%\tgrind[htbp]{code/be.s.tex}{Block Exponent}{opt:be}

% This is done by using some combination of
%\begin{eqnarray*}
%a_i & = & a_j + a_k \\
%a_i & = & 2a_j + a_k \\
%a_i & = & 4a_j + a_k \\
%a_i & = & 8a_j + a_k \\
%a_i & = & a_j - a_k \\
%a_i & = & a_j \ll m \mbox{shift}
%\end{eqnarray*}
%instead of the multiplication.  For example, to multiply $s$ by 10 and storethe result in $r$, you could use:
%\begin{eqnarray*}
%r & = & 4s + s\\
%r & = & r + r
%\end{eqnarray*}
%Or by 59:
%\begin{eqnarray*}
%t & = & 2s + s \\
%r & = & 2t + s \\
%r & = & 8r + t
%\end{eqnarray*}


%Many pairs of bases are known to be incoherent, so for example the Fourier and Wavelet bases [10]. In particular a basis built from i.i.d. random draws from a Gaussian or a Bernoulli distribution will be incoherent with any other xed basis with a high probability in high dimensions.
